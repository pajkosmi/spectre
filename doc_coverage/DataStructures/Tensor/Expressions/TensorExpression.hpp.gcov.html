<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - 4d65226095c9cbdadb551919684c45208a02b43c - DataStructures/Tensor/Expressions/TensorExpression.hpp</title>
  <link rel="stylesheet" type="text/css" href="../../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title"> SpECTRE Documentation Coverage Report</td></tr>
    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../../index.html">top level</a> - <a href="index.html">DataStructures/Tensor/Expressions</a> - TensorExpression.hpp</td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
             <td class="headerItem">Commit:</td>
             <td class="headerValue"><a target="_blank" href="https://github.com/sxs-collaboration/spectre/commit/4d65226095c9cbdadb551919684c45208a02b43c">4d65226095c9cbdadb551919684c45208a02b43c</a></td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">9</td>
            <td class="headerCovTableEntry">10</td>
            <td class="headerCovTableEntryHi">90.0 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2023-06-19 22:29:53</td>
            <td></td>
          </tr>
          <tr>
            <td class="headerItem">Legend:</td>
            <td class="headerValueLeg">            Lines:
            <span class="coverLegendCov">hit</span>
            <span class="coverLegendNoCov">not hit</span>
</td>
            <td></td>
          </tr>
          <tr><td><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span><span class="lineCov">          1 : // Distributed under the MIT License.</span></a>
<a name="2"><span class="lineNum">       2 </span>            : // See LICENSE.txt for details.</a>
<a name="3"><span class="lineNum">       3 </span>            : </a>
<a name="4"><span class="lineNum">       4 </span>            : /// \file</a>
<a name="5"><span class="lineNum">       5 </span>            : /// Defines base class for all tensor expressions</a>
<a name="6"><span class="lineNum">       6 </span>            : </a>
<a name="7"><span class="lineNum">       7 </span>            : #pragma once</a>
<a name="8"><span class="lineNum">       8 </span>            : </a>
<a name="9"><span class="lineNum">       9 </span>            : #include &lt;limits&gt;</a>
<a name="10"><span class="lineNum">      10 </span>            : </a>
<a name="11"><span class="lineNum">      11 </span>            : #include &quot;Utilities/ForceInline.hpp&quot;</a>
<a name="12"><span class="lineNum">      12 </span>            : #include &quot;Utilities/TMPL.hpp&quot;</a>
<a name="13"><span class="lineNum">      13 </span>            : </a>
<a name="14"><span class="lineNum">      14 </span>            : /// \ingroup TensorExpressionsGroup</a>
<a name="15"><span class="lineNum">      15 </span>            : /// \brief Marks a class as being a TensorExpression</a>
<a name="16"><span class="lineNum">      16 </span>            : ///</a>
<a name="17"><span class="lineNum">      17 </span>            : /// \details</a>
<a name="18"><span class="lineNum">      18 </span>            : /// The empty base class provides a simple means for checking if a type is a</a>
<a name="19"><span class="lineNum">      19 </span>            : /// TensorExpression.</a>
<a name="20"><span class="lineNum">      20 </span><span class="lineCov">          1 : struct Expression {};</span></a>
<a name="21"><span class="lineNum">      21 </span>            : </a>
<a name="22"><span class="lineNum">      22 </span>            : /// @{</a>
<a name="23"><span class="lineNum">      23 </span>            : /// \ingroup TensorExpressionsGroup</a>
<a name="24"><span class="lineNum">      24 </span>            : /// \brief The base class all tensor expression implementations derive from</a>
<a name="25"><span class="lineNum">      25 </span>            : ///</a>
<a name="26"><span class="lineNum">      26 </span>            : /// \details</a>
<a name="27"><span class="lineNum">      27 </span>            : /// ## Tensor equation construction</a>
<a name="28"><span class="lineNum">      28 </span>            : /// Each derived `TensorExpression` class should be thought of as an expression</a>
<a name="29"><span class="lineNum">      29 </span>            : /// tree that represents some operation done on or between tensor expressions.</a>
<a name="30"><span class="lineNum">      30 </span>            : /// Arithmetic operators and other mathematical functions of interest</a>
<a name="31"><span class="lineNum">      31 </span>            : /// (e.g. `sqrt`) have overloads defined that accept `TensorExpression`s and</a>
<a name="32"><span class="lineNum">      32 </span>            : /// return a new `TensorExpression` representing the result tensor of such an</a>
<a name="33"><span class="lineNum">      33 </span>            : /// operation. In this way, an equation written with `TensorExpression`s will</a>
<a name="34"><span class="lineNum">      34 </span>            : /// generate an expression tree where the internal and leaf nodes are instances</a>
<a name="35"><span class="lineNum">      35 </span>            : /// of the derived `TensorExpression` classes. For example, `tenex::AddSub`</a>
<a name="36"><span class="lineNum">      36 </span>            : /// defines an internal node for handling the addition and subtraction</a>
<a name="37"><span class="lineNum">      37 </span>            : /// operations between tensors expressions, while `tenex::TensorAsExpression`</a>
<a name="38"><span class="lineNum">      38 </span>            : /// defines a leaf node that represents a single `Tensor` that appears in the</a>
<a name="39"><span class="lineNum">      39 </span>            : /// equation.</a>
<a name="40"><span class="lineNum">      40 </span>            : ///</a>
<a name="41"><span class="lineNum">      41 </span>            : /// ## Tensor equation evaluation</a>
<a name="42"><span class="lineNum">      42 </span>            : /// The overall tree for an equation and the order in which we traverse the tree</a>
<a name="43"><span class="lineNum">      43 </span>            : /// define the order of operations done to compute the resulting LHS `Tensor`.</a>
<a name="44"><span class="lineNum">      44 </span>            : /// The evaluation is done by `tenex::evaluate`, which traverses the whole tree</a>
<a name="45"><span class="lineNum">      45 </span>            : /// once for each unique LHS component in order to evaluate the full LHS</a>
<a name="46"><span class="lineNum">      46 </span>            : /// `Tensor`. There are two different traversals currently implemented that are</a>
<a name="47"><span class="lineNum">      47 </span>            : /// chosen from, depending on the tensor equation being evaluated:</a>
<a name="48"><span class="lineNum">      48 </span>            : /// 1. **Evaluate the whole tree as one expression** using in-order traversal.</a>
<a name="49"><span class="lineNum">      49 </span>            : /// This is like generating and solving a one-liner of the whole equation.</a>
<a name="50"><span class="lineNum">      50 </span>            : /// 2. **Split up the tree into subexpressions** that are each evaluated with</a>
<a name="51"><span class="lineNum">      51 </span>            : /// in-order traversal to successively &quot;accumulate&quot; a LHS result component of</a>
<a name="52"><span class="lineNum">      52 </span>            : /// the equation. This is like splitting the equation up and solving pieces of</a>
<a name="53"><span class="lineNum">      53 </span>            : /// it at a time with multiple lines of assignments/updates (see details below).</a>
<a name="54"><span class="lineNum">      54 </span>            : ///</a>
<a name="55"><span class="lineNum">      55 </span>            : /// ## Equation splitting details</a>
<a name="56"><span class="lineNum">      56 </span>            : /// Splitting up the tree and evaluating subexpressions is beneficial when we</a>
<a name="57"><span class="lineNum">      57 </span>            : /// believe it to lead to a better runtime than if we were to compute the whole</a>
<a name="58"><span class="lineNum">      58 </span>            : /// expression as a one-liner. One important use case is when the `Tensor`s in</a>
<a name="59"><span class="lineNum">      59 </span>            : /// the equation hold components whose data type is `DataVector`. From</a>
<a name="60"><span class="lineNum">      60 </span>            : /// benchmarking, it was found that the runtime of `DataVector` expressions</a>
<a name="61"><span class="lineNum">      61 </span>            : /// scales poorly as we increase the number of operations. For example, for an</a>
<a name="62"><span class="lineNum">      62 </span>            : /// inner product with 256 sums of products, instead of adding 256 `DataVector`</a>
<a name="63"><span class="lineNum">      63 </span>            : /// products in one line (e.g. `result = A*B + C*D + E*F + ...;`), it's much</a>
<a name="64"><span class="lineNum">      64 </span>            : /// faster to, say, set the result to be the sum of the first 8 products, then</a>
<a name="65"><span class="lineNum">      65 </span>            : /// `+=` the next 8, and so forth. This is what is meant by &quot;accumulating&quot; the</a>
<a name="66"><span class="lineNum">      66 </span>            : /// LHS result tensor, and what the `TensorExpression` splitting emulates. Note</a>
<a name="67"><span class="lineNum">      67 </span>            : /// that while 8 is the number used in this example, the exact optimal number of</a>
<a name="68"><span class="lineNum">      68 </span>            : /// operations will be hardware-dependent, but probably not something we need to</a>
<a name="69"><span class="lineNum">      69 </span>            : /// really worry about fine-tuning. However, a ballpark estimate for a &quot;good&quot;</a>
<a name="70"><span class="lineNum">      70 </span>            : /// number of operations may vary greatly depending on the data type of the</a>
<a name="71"><span class="lineNum">      71 </span>            : /// components (e.g. `double` vs. `DataVector`), which is something important</a>
<a name="72"><span class="lineNum">      72 </span>            : /// to at least coarsely tune.</a>
<a name="73"><span class="lineNum">      73 </span>            : ///</a>
<a name="74"><span class="lineNum">      74 </span>            : /// ### How the tree is split up</a>
<a name="75"><span class="lineNum">      75 </span>            : /// Let's define the **primary path** to be the path in the tree going from the</a>
<a name="76"><span class="lineNum">      76 </span>            : /// root node to the leftmost leaf. The overall tree contains subtrees</a>
<a name="77"><span class="lineNum">      77 </span>            : /// represented by different `TensorExpression`s in the equation. Certain</a>
<a name="78"><span class="lineNum">      78 </span>            : /// subtrees are marked as the starting and/or ending points of these &quot;pieces&quot;</a>
<a name="79"><span class="lineNum">      79 </span>            : /// of the equation. Let's define a **leg** to be a &quot;segment&quot; along the primary</a>
<a name="80"><span class="lineNum">      80 </span>            : /// path delineated by a starting and ending expression subtree. These</a>
<a name="81"><span class="lineNum">      81 </span>            : /// delineations are made where we decide there are enough operations in a</a>
<a name="82"><span class="lineNum">      82 </span>            : /// subtree that it would be wise to split at that point. What is considered to</a>
<a name="83"><span class="lineNum">      83 </span>            : /// be &quot;enough&quot; operations is specialized based on the data type held by the</a>
<a name="84"><span class="lineNum">      84 </span>            : /// `Tensor`s in the expression (see `tenex::max_num_ops_in_sub_expression`).</a>
<a name="85"><span class="lineNum">      85 </span>            : ///</a>
<a name="86"><span class="lineNum">      86 </span>            : /// ### How a split tree is traversed and evaluated</a>
<a name="87"><span class="lineNum">      87 </span>            : /// We recurse down the primary path, visiting each expression subtree until we</a>
<a name="88"><span class="lineNum">      88 </span>            : /// reach the start of the lowest leg, then initialize the LHS result component</a>
<a name="89"><span class="lineNum">      89 </span>            : /// we're wanting to compute to be the result of this lowest expression. Then,</a>
<a name="90"><span class="lineNum">      90 </span>            : /// we recurse back up to the expression subtree that is starting point of the</a>
<a name="91"><span class="lineNum">      91 </span>            : /// leg &quot;above&quot; it and compute that subtree. This time, however, when</a>
<a name="92"><span class="lineNum">      92 </span>            : /// recursively evaluating this higher subtree, we substitute in the current LHS</a>
<a name="93"><span class="lineNum">      93 </span>            : /// result for that lower subtree that we have already computed. This is</a>
<a name="94"><span class="lineNum">      94 </span>            : /// repeated as we &quot;climb up&quot; the primary path to successively accumulate the</a>
<a name="95"><span class="lineNum">      95 </span>            : /// result component.</a>
<a name="96"><span class="lineNum">      96 </span>            : ///</a>
<a name="97"><span class="lineNum">      97 </span>            : /// **Note:** The primary path is currently implemented as the path specified</a>
<a name="98"><span class="lineNum">      98 </span>            : /// above, but there's no reason it couldn't be reimplemented to be a different</a>
<a name="99"><span class="lineNum">      99 </span>            : /// path. The idea with the current implementation is to select a path from root</a>
<a name="100"><span class="lineNum">     100 </span>            : /// to leaf that is long so we have more flexibility in splitting, should we</a>
<a name="101"><span class="lineNum">     101 </span>            : /// want to. When evaluating, we *could* implement the traversal to take a</a>
<a name="102"><span class="lineNum">     102 </span>            : /// different path, but currently, derived `TensorExpression`s that represent</a>
<a name="103"><span class="lineNum">     103 </span>            : /// commutative binary operations are instantiated with the larger subtree being</a>
<a name="104"><span class="lineNum">     104 </span>            : /// the left child and the smaller subtree being the right child. By</a>
<a name="105"><span class="lineNum">     105 </span>            : /// constructing it this way, we elongate the leftmost path, which will allow</a>
<a name="106"><span class="lineNum">     106 </span>            : /// for increased splitting.</a>
<a name="107"><span class="lineNum">     107 </span>            : ///</a>
<a name="108"><span class="lineNum">     108 </span>            : /// ## Requirements for derived `TensorExpression` classes</a>
<a name="109"><span class="lineNum">     109 </span>            : /// Each derived `TensorExpression` class must define the following aliases and</a>
<a name="110"><span class="lineNum">     110 </span>            : /// members:</a>
<a name="111"><span class="lineNum">     111 </span>            : /// - `private` variables that store its operands' derived `TensorExpression`s.</a>
<a name="112"><span class="lineNum">     112 </span>            : /// We make these non-`const` to allow for move construction.</a>
<a name="113"><span class="lineNum">     113 </span>            : /// - Constructor that initializes the above `private` operand members</a>
<a name="114"><span class="lineNum">     114 </span>            : /// - alias `type`: The data type of the data being stored in the result of the</a>
<a name="115"><span class="lineNum">     115 </span>            : /// expression, e.g. `double`, `DataVector`</a>
<a name="116"><span class="lineNum">     116 </span>            : /// - alias `symmetry`: The ::Symmetry of the result of the expression</a>
<a name="117"><span class="lineNum">     117 </span>            : /// - alias `index_list`: The list of \ref SpacetimeIndex &quot;TensorIndexType&quot;s of</a>
<a name="118"><span class="lineNum">     118 </span>            : /// the result of the expression</a>
<a name="119"><span class="lineNum">     119 </span>            : /// - alias `args_list`: The list of generic `TensorIndex`s of the result of the</a>
<a name="120"><span class="lineNum">     120 </span>            : /// expression</a>
<a name="121"><span class="lineNum">     121 </span>            : /// - variable `static constexpr size_t num_tensor_indices`: The number of</a>
<a name="122"><span class="lineNum">     122 </span>            : /// tensor indices in the result of the expression</a>
<a name="123"><span class="lineNum">     123 </span>            : /// - variable `static constexpr size_t num_ops_left_child`: The number of</a>
<a name="124"><span class="lineNum">     124 </span>            : /// arithmetic tensor operations done in the subtree for the expression's left</a>
<a name="125"><span class="lineNum">     125 </span>            : /// operand. If the expression represents a unary operation, their only child is</a>
<a name="126"><span class="lineNum">     126 </span>            : /// considered the left child. If the expression is a leaf node, then this value</a>
<a name="127"><span class="lineNum">     127 </span>            : /// should be set to 0 since retrieving a value at the leaf involves 0</a>
<a name="128"><span class="lineNum">     128 </span>            : /// arithmetic tensor operations.</a>
<a name="129"><span class="lineNum">     129 </span>            : /// - variable `static constexpr size_t num_ops_right_child`: The number of</a>
<a name="130"><span class="lineNum">     130 </span>            : /// arithmetic tensor operations done in the expression's right operand. If the</a>
<a name="131"><span class="lineNum">     131 </span>            : /// expression represents a unary operation or is leaf node, this should be set</a>
<a name="132"><span class="lineNum">     132 </span>            : /// to 0 because there is no right child.</a>
<a name="133"><span class="lineNum">     133 </span>            : /// - variable `static constexpr size_t num_ops_subtree`: The number of</a>
<a name="134"><span class="lineNum">     134 </span>            : /// arithmetic tensor operations done in the subtree represented by the</a>
<a name="135"><span class="lineNum">     135 </span>            : /// expression. For `AddSub`, for example, this is</a>
<a name="136"><span class="lineNum">     136 </span>            : /// `num_ops_left_child + num_ops_right_child + 1`, the sum of the number of</a>
<a name="137"><span class="lineNum">     137 </span>            : /// operations in each operand's subtrees plus one for the operation done for</a>
<a name="138"><span class="lineNum">     138 </span>            : /// the expression, itself.</a>
<a name="139"><span class="lineNum">     139 </span>            : /// - variable `static constexpr size_t</a>
<a name="140"><span class="lineNum">     140 </span>            : /// height_relative_to_closest_tensor_leaf_in_subtree` : The height of an</a>
<a name="141"><span class="lineNum">     141 </span>            : /// expression's node in the overall expression tree relative to the closest</a>
<a name="142"><span class="lineNum">     142 </span>            : /// `TensorAsExpression` leaf in its subtree. This is stored so that we can</a>
<a name="143"><span class="lineNum">     143 </span>            : /// traverse from the root along the shortest path to a `Tensor` when</a>
<a name="144"><span class="lineNum">     144 </span>            : /// retrieving the size of a component from the RHS expression (see</a>
<a name="145"><span class="lineNum">     145 </span>            : /// `get_rhs_tensor_component_size()` below). Non-`Tensor` leaves (e.g.</a>
<a name="146"><span class="lineNum">     146 </span>            : /// `NumberAsExpression`) are defined to have maximum height</a>
<a name="147"><span class="lineNum">     147 </span>            : /// `std::numeric_limits&lt;size_t&gt;::max()` to encode that they are maximally</a>
<a name="148"><span class="lineNum">     148 </span>            : /// far away from their nearest `Tensor` descendant, since the expression's</a>
<a name="149"><span class="lineNum">     149 </span>            : /// subtree (a leaf) can never have a `TensorAsExpression` descedant from it.</a>
<a name="150"><span class="lineNum">     150 </span>            : /// This maximal height is leveraged by `get_rhs_tensor_component_size()` so</a>
<a name="151"><span class="lineNum">     151 </span>            : /// that in traversing the expression tree to find a `Tensor`, it will never</a>
<a name="152"><span class="lineNum">     152 </span>            : /// take the path that ends in a non-`Tensor` leaf because it is the worst path</a>
<a name="153"><span class="lineNum">     153 </span>            : /// option.</a>
<a name="154"><span class="lineNum">     154 </span>            : /// - function `decltype(auto) get(const std::array&lt;size_t, num_tensor_indices&gt;&amp;</a>
<a name="155"><span class="lineNum">     155 </span>            : /// result_multi_index) const`: Accepts a multi-index for the result tensor</a>
<a name="156"><span class="lineNum">     156 </span>            : /// represented by the expression and returns the computed result of the</a>
<a name="157"><span class="lineNum">     157 </span>            : /// expression at that multi-index. This should call the operands' `get`</a>
<a name="158"><span class="lineNum">     158 </span>            : /// functions in order to recursively compute the result of the expression.</a>
<a name="159"><span class="lineNum">     159 </span>            : /// - function template</a>
<a name="160"><span class="lineNum">     160 </span>            : /// `template &lt;typename LhsTensor&gt; void assert_lhs_tensor_not_in_rhs_expression(</a>
<a name="161"><span class="lineNum">     161 </span>            : /// const gsl::not_null&lt;LhsTensor*&gt; lhs_tensor) const`: Asserts that the LHS</a>
<a name="162"><span class="lineNum">     162 </span>            : /// `Tensor` we're computing does not also appear in the RHS `TensorExpression`.</a>
<a name="163"><span class="lineNum">     163 </span>            : /// We define this because if a tree is split up, then the LHS `Tensor` will</a>
<a name="164"><span class="lineNum">     164 </span>            : /// generally not be computed correctly because the LHS components will be</a>
<a name="165"><span class="lineNum">     165 </span>            : /// updated as we traverse the split tree.</a>
<a name="166"><span class="lineNum">     166 </span>            : /// - function template</a>
<a name="167"><span class="lineNum">     167 </span>            : /// \code</a>
<a name="168"><span class="lineNum">     168 </span>            : /// template &lt;typename LhsTensorIndices, typename LhsTensor&gt;</a>
<a name="169"><span class="lineNum">     169 </span>            : /// void assert_lhs_tensorindices_same_in_rhs(</a>
<a name="170"><span class="lineNum">     170 </span>            : ///     const gsl::not_null&lt;LhsTensor*&gt; lhs_tensor) const;</a>
<a name="171"><span class="lineNum">     171 </span>            : /// \endcode</a>
<a name="172"><span class="lineNum">     172 </span>            : /// Asserts that any instance of the LHS `Tensor` in the RHS `TensorExpression`</a>
<a name="173"><span class="lineNum">     173 </span>            : /// uses the same generic index order that the LHS uses. We define this because</a>
<a name="174"><span class="lineNum">     174 </span>            : /// if a tree is not split up, it's safe to use the LHS `Tensor` on the RHS if</a>
<a name="175"><span class="lineNum">     175 </span>            : /// the generic index order is the same. In these cases, `tenex::update` should</a>
<a name="176"><span class="lineNum">     176 </span>            : /// be used instead of `tenex::evaluate`. See the documentation for</a>
<a name="177"><span class="lineNum">     177 </span>            : /// `tenex::update` for more details and `tenex::detail::evaluate_impl` for why</a>
<a name="178"><span class="lineNum">     178 </span>            : /// this is safe to do.</a>
<a name="179"><span class="lineNum">     179 </span>            : /// - function `size_t get_rhs_tensor_component_size() const`: Gets the size of</a>
<a name="180"><span class="lineNum">     180 </span>            : /// a component from a `Tensor` in an expression's subtree of the RHS</a>
<a name="181"><span class="lineNum">     181 </span>            : /// expression. This is used to size LHS components, if needed. Utilizes</a>
<a name="182"><span class="lineNum">     182 </span>            : /// `height_relative_to_closest_tensor_leaf_in_subtree` to recursively find the</a>
<a name="183"><span class="lineNum">     183 </span>            : /// nearest `TensorAsExpression` descendant leaf.</a>
<a name="184"><span class="lineNum">     184 </span>            : ///</a>
<a name="185"><span class="lineNum">     185 </span>            : /// Each derived `TensorExpression` class must also define the following</a>
<a name="186"><span class="lineNum">     186 </span>            : /// members, which have real meaning for the expression *only* if it ends up</a>
<a name="187"><span class="lineNum">     187 </span>            : /// belonging to the primary path of the tree that is traversed:</a>
<a name="188"><span class="lineNum">     188 </span>            : /// - variable `static constexpr bool is_primary_start`: If on the primary path,</a>
<a name="189"><span class="lineNum">     189 </span>            : /// whether or not the expression is a starting point of a leg. This is true</a>
<a name="190"><span class="lineNum">     190 </span>            : /// when there are enough operations to warrant splitting (see</a>
<a name="191"><span class="lineNum">     191 </span>            : /// `tenex::max_num_ops_in_sub_expression`).</a>
<a name="192"><span class="lineNum">     192 </span>            : /// - variable `static constexpr bool is_primary_end`: If on the primary path,</a>
<a name="193"><span class="lineNum">     193 </span>            : /// whether or not the expression is an ending point of a leg. This is true when</a>
<a name="194"><span class="lineNum">     194 </span>            : /// the expression's child along the primary path is a starting point of a leg.</a>
<a name="195"><span class="lineNum">     195 </span>            : /// - variable `static constexpr size_t num_ops_to_evaluate_primary_left_child`:</a>
<a name="196"><span class="lineNum">     196 </span>            : /// If on the primary path, this is the remaining number of arithmetic tensor</a>
<a name="197"><span class="lineNum">     197 </span>            : /// operations that need to be done in the subtree of the child along the</a>
<a name="198"><span class="lineNum">     198 </span>            : /// primary path, given that we will have already computed the whole subtree at</a>
<a name="199"><span class="lineNum">     199 </span>            : /// the next lowest leg's starting point.</a>
<a name="200"><span class="lineNum">     200 </span>            : /// - variable</a>
<a name="201"><span class="lineNum">     201 </span>            : /// `static constexpr size_t num_ops_to_evaluate_primary_right_child`:</a>
<a name="202"><span class="lineNum">     202 </span>            : /// If on the primary path, this is the remaining number of arithmetic tensor</a>
<a name="203"><span class="lineNum">     203 </span>            : /// operations that need to be done in the right operand's subtree. Because</a>
<a name="204"><span class="lineNum">     204 </span>            : /// the branches off of the primary path currently are not split up in any way,</a>
<a name="205"><span class="lineNum">     205 </span>            : /// this currently should simply be equal to `num_ops_right_child`. If logic is</a>
<a name="206"><span class="lineNum">     206 </span>            : /// added to split up these branches, logic will need to be added to compute</a>
<a name="207"><span class="lineNum">     207 </span>            : /// this remaining number of operations in the right subtree.</a>
<a name="208"><span class="lineNum">     208 </span>            : /// - variable `static constexpr size_t num_ops_to_evaluate_primary_subtree`:</a>
<a name="209"><span class="lineNum">     209 </span>            : /// If on the primary path, this is the remaining number of arithmetic tensor</a>
<a name="210"><span class="lineNum">     210 </span>            : /// operations that need to be done for this expression's subtree, given that we</a>
<a name="211"><span class="lineNum">     211 </span>            : /// will have already computed the subtree at the next lowest leg's starting</a>
<a name="212"><span class="lineNum">     212 </span>            : /// point. For example, for `tenex::AddSub`, this is just</a>
<a name="213"><span class="lineNum">     213 </span>            : /// `num_ops_to_evaluate_primary_left_child +</a>
<a name="214"><span class="lineNum">     214 </span>            : /// num_ops_to_evaluate_primary_right_child + 1` (the extra 1 for the `+` or `-`</a>
<a name="215"><span class="lineNum">     215 </span>            : /// operation itself).</a>
<a name="216"><span class="lineNum">     216 </span>            : /// - variable</a>
<a name="217"><span class="lineNum">     217 </span>            : /// `static constexpr bool primary_child_subtree_contains_primary_start`:</a>
<a name="218"><span class="lineNum">     218 </span>            : /// If on the primary path, whether or not the expression's child along the</a>
<a name="219"><span class="lineNum">     219 </span>            : /// primary path is a subtree that contains a starting point of a leg along the</a>
<a name="220"><span class="lineNum">     220 </span>            : /// primary path. In other words, whether or not there is a split on the primary</a>
<a name="221"><span class="lineNum">     221 </span>            : /// path lower than this expression. When evaluating a split tree, this is</a>
<a name="222"><span class="lineNum">     222 </span>            : /// useful because it tells us we need to keep recursing down to a lower leg and</a>
<a name="223"><span class="lineNum">     223 </span>            : /// evaluate that lower subtree first before evaluating the current subtree.</a>
<a name="224"><span class="lineNum">     224 </span>            : /// - variable `static constexpr bool primary_subtree_contains_primary_start`:</a>
<a name="225"><span class="lineNum">     225 </span>            : /// If on the primary path, whether or not this subtree contains a starting</a>
<a name="226"><span class="lineNum">     226 </span>            : /// point of a leg along the primary path. In other words, whether or not there</a>
<a name="227"><span class="lineNum">     227 </span>            : /// is a split on the primary path at this expression or beneath it.</a>
<a name="228"><span class="lineNum">     228 </span>            : /// - function template</a>
<a name="229"><span class="lineNum">     229 </span>            : /// \code</a>
<a name="230"><span class="lineNum">     230 </span>            : /// template &lt;typename ResultType&gt;</a>
<a name="231"><span class="lineNum">     231 </span>            : /// decltype(auto) get_primary(</a>
<a name="232"><span class="lineNum">     232 </span>            : ///     ResultType&amp; result_component,</a>
<a name="233"><span class="lineNum">     233 </span>            : ///     const std::array&lt;size_t, num_tensor_indices&gt;&amp; result_multi_index) const</a>
<a name="234"><span class="lineNum">     234 </span>            : /// \endcode</a>
<a name="235"><span class="lineNum">     235 </span>            : /// This is similar to the required `get` function described above, but this</a>
<a name="236"><span class="lineNum">     236 </span>            : /// should be used when the tree is split up. The main difference with this</a>
<a name="237"><span class="lineNum">     237 </span>            : /// function is that it takes the current result component (that we're</a>
<a name="238"><span class="lineNum">     238 </span>            : /// computing) as an argument, and when we hit the starting point of the next</a>
<a name="239"><span class="lineNum">     239 </span>            : /// lowest leg on the primary path when recursively evaluating the current leg,</a>
<a name="240"><span class="lineNum">     240 </span>            : /// we substitute in the current LHS result for the subtree that we have already</a>
<a name="241"><span class="lineNum">     241 </span>            : /// computed. This function should call `get_primary` on the child on the</a>
<a name="242"><span class="lineNum">     242 </span>            : /// primary path and `get` on the other child, if one exists.</a>
<a name="243"><span class="lineNum">     243 </span>            : /// - function template</a>
<a name="244"><span class="lineNum">     244 </span>            : /// \code</a>
<a name="245"><span class="lineNum">     245 </span>            : /// template &lt;typename ResultType&gt;</a>
<a name="246"><span class="lineNum">     246 </span>            : /// void evaluate_primary_subtree(</a>
<a name="247"><span class="lineNum">     247 </span>            : ///     ResultType&amp; result_component,</a>
<a name="248"><span class="lineNum">     248 </span>            : ///     const std::array&lt;size_t, num_tensor_indices&gt;&amp; result_multi_index) const</a>
<a name="249"><span class="lineNum">     249 </span>            : /// \endcode</a>
<a name="250"><span class="lineNum">     250 </span>            : /// This should first recursively evaluate the legs beneath it on the primary</a>
<a name="251"><span class="lineNum">     251 </span>            : /// path, then if the expression itself is the start of a leg, it should</a>
<a name="252"><span class="lineNum">     252 </span>            : /// evaluate this leg by calling the expression's own `get_primary` to compute</a>
<a name="253"><span class="lineNum">     253 </span>            : /// it and update the result component being accumulated. `tenex::evaluate`</a>
<a name="254"><span class="lineNum">     254 </span>            : /// should call this function on the root node for the whole tree if there is</a>
<a name="255"><span class="lineNum">     255 </span>            : /// determined to be any splits in the tree.</a>
<a name="256"><span class="lineNum">     256 </span>            : ///</a>
<a name="257"><span class="lineNum">     257 </span>            : /// ## Data type support</a>
<a name="258"><span class="lineNum">     258 </span>            : /// Which types can be used, which operations with which types can be performed,</a>
<a name="259"><span class="lineNum">     259 </span>            : /// and other type-specific support and configuration can be found in</a>
<a name="260"><span class="lineNum">     260 </span>            : /// `DataStructures/Tensor/Expressions/DataTypeSupport.hpp`. To add support for</a>
<a name="261"><span class="lineNum">     261 </span>            : /// equation terms with a certain type or to modify the configuration for a</a>
<a name="262"><span class="lineNum">     262 </span>            : /// type that is already supported, see the contents of that file and modify</a>
<a name="263"><span class="lineNum">     263 </span>            : /// settings as necessary.</a>
<a name="264"><span class="lineNum">     264 </span>            : ///</a>
<a name="265"><span class="lineNum">     265 </span>            : /// ## Current advice for improving and extending `TensorExpression`s</a>
<a name="266"><span class="lineNum">     266 </span>            : /// - Derived `TensorExpression` classes (or the overloads that produce them)</a>
<a name="267"><span class="lineNum">     267 </span>            : /// should include `static_assert`s for ensuring mathematical correctness</a>
<a name="268"><span class="lineNum">     268 </span>            : /// wherever reasonable</a>
<a name="269"><span class="lineNum">     269 </span>            : /// - Minimize breadth in the tree where possible because benchmarking inner</a>
<a name="270"><span class="lineNum">     270 </span>            : /// products has shown that increased tree breadth can cause slower runtimes.</a>
<a name="271"><span class="lineNum">     271 </span>            : /// In addition, more breadth means a decreased ability to split up the tree</a>
<a name="272"><span class="lineNum">     272 </span>            : /// along the primary path.</a>
<a name="273"><span class="lineNum">     273 </span>            : /// - Minimize the number of multi-index transformations that need to be done</a>
<a name="274"><span class="lineNum">     274 </span>            : /// when evaluating the tree. For some operations like addition, the associated</a>
<a name="275"><span class="lineNum">     275 </span>            : /// multi-indices of the two operands needs to be computed from the multi-index</a>
<a name="276"><span class="lineNum">     276 </span>            : /// of the result, which may involve reordering and/or shifting the values of</a>
<a name="277"><span class="lineNum">     277 </span>            : /// the result index. It's good to minimize the number of these kinds of</a>
<a name="278"><span class="lineNum">     278 </span>            : /// transformations from result to operand multi-index where we can.</a>
<a name="279"><span class="lineNum">     279 </span>            : /// - Unless the implementation of Tensor_detail::Structure changes, it's not</a>
<a name="280"><span class="lineNum">     280 </span>            : /// advised for the derived `TensorExpression` classes to have anything that</a>
<a name="281"><span class="lineNum">     281 </span>            : /// would instantiate the Tensor_detail::Structure of the tensor that would</a>
<a name="282"><span class="lineNum">     282 </span>            : /// result from the expression. This is really only a problem when the result of</a>
<a name="283"><span class="lineNum">     283 </span>            : /// the expression would be a tensor with many components, because the compile</a>
<a name="284"><span class="lineNum">     284 </span>            : /// time of the mapping between storage indices and multi-indices within</a>
<a name="285"><span class="lineNum">     285 </span>            : /// Tensor_detail::Structure scales very poorly with the number of components.</a>
<a name="286"><span class="lineNum">     286 </span>            : /// It's important to keep in mind that while SpECTRE currently only supports</a>
<a name="287"><span class="lineNum">     287 </span>            : /// creating `Tensor`s up to rank 4, there is nothing preventing the represented</a>
<a name="288"><span class="lineNum">     288 </span>            : /// result tensor of a expression being higher rank, e.g.</a>
<a name="289"><span class="lineNum">     289 </span>            : /// `R(ti_j, ti_b, ti_A) * (S(ti_d, ti_a, ti_B, ti_C) * T(ti_J, ti_k, ti_l))`</a>
<a name="290"><span class="lineNum">     290 </span>            : /// contains an intermediate outer product expression</a>
<a name="291"><span class="lineNum">     291 </span>            : /// `S(ti_d, ti_a, ti_B, ti_C) * T(ti_J, ti_k, ti_l)` that represents a rank 7</a>
<a name="292"><span class="lineNum">     292 </span>            : /// tensor, even though a rank 7 `Tensor` is never instantiated. Having the</a>
<a name="293"><span class="lineNum">     293 </span>            : /// outer product expression instantiate the Tensor_detail::Structure of this</a>
<a name="294"><span class="lineNum">     294 </span>            : /// intermediate result currently leads to an unreasonable compile time.</a>
<a name="295"><span class="lineNum">     295 </span>            : ///</a>
<a name="296"><span class="lineNum">     296 </span>            : /// \tparam Derived the derived class needed for</a>
<a name="297"><span class="lineNum">     297 </span>            : /// [CRTP](https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern)</a>
<a name="298"><span class="lineNum">     298 </span>            : /// \tparam DataType the type of the data being stored in the `Tensor`s</a>
<a name="299"><span class="lineNum">     299 </span>            : /// \tparam Symm the ::Symmetry of the Derived class</a>
<a name="300"><span class="lineNum">     300 </span>            : /// \tparam IndexList the list of \ref SpacetimeIndex &quot;TensorIndexType&quot;s</a>
<a name="301"><span class="lineNum">     301 </span>            : /// \tparam Args typelist of the tensor indices, e.g. types of `ti::a` and</a>
<a name="302"><span class="lineNum">     302 </span>            : /// `ti::b` in `F(ti::a, ti::b)`</a>
<a name="303"><span class="lineNum">     303 </span>            : /// \cond HIDDEN_SYMBOLS</a>
<a name="304"><span class="lineNum">     304 </span>            : template &lt;typename Derived, typename DataType, typename Symm,</a>
<a name="305"><span class="lineNum">     305 </span>            :           typename IndexList, typename Args = tmpl::list&lt;&gt;,</a>
<a name="306"><span class="lineNum">     306 </span>            :           typename ReducedArgs = tmpl::list&lt;&gt;&gt;</a>
<a name="307"><span class="lineNum">     307 </span>            : struct TensorExpression;</a>
<a name="308"><span class="lineNum">     308 </span>            : /// \endcond</a>
<a name="309"><span class="lineNum">     309 </span>            : </a>
<a name="310"><span class="lineNum">     310 </span>            : template &lt;typename Derived, typename DataType, typename Symm,</a>
<a name="311"><span class="lineNum">     311 </span>            :           typename... Indices, template &lt;typename...&gt; class ArgsList,</a>
<a name="312"><span class="lineNum">     312 </span>            :           typename... Args&gt;</a>
<a name="313"><span class="lineNum">     313 </span><span class="lineCov">          1 : struct TensorExpression&lt;Derived, DataType, Symm, tmpl::list&lt;Indices...&gt;,</span></a>
<a name="314"><span class="lineNum">     314 </span>            :                         ArgsList&lt;Args...&gt;&gt; : public Expression {</a>
<a name="315"><span class="lineNum">     315 </span>            :   static_assert(sizeof...(Args) == 0 or sizeof...(Args) == sizeof...(Indices),</a>
<a name="316"><span class="lineNum">     316 </span>            :                 &quot;the number of Tensor indices must match the number of &quot;</a>
<a name="317"><span class="lineNum">     317 </span>            :                 &quot;components specified in an expression.&quot;);</a>
<a name="318"><span class="lineNum">     318 </span>            :   /// The type of the data being stored in the `Tensor`s</a>
<a name="319"><span class="lineNum">     319 </span><span class="lineCov">          1 :   using type = DataType;</span></a>
<a name="320"><span class="lineNum">     320 </span>            :   /// The ::Symmetry of the `Derived` class</a>
<a name="321"><span class="lineNum">     321 </span><span class="lineCov">          1 :   using symmetry = Symm;</span></a>
<a name="322"><span class="lineNum">     322 </span>            :   /// The list of \ref SpacetimeIndex &quot;TensorIndexType&quot;s</a>
<a name="323"><span class="lineNum">     323 </span><span class="lineCov">          1 :   using index_list = tmpl::list&lt;Indices...&gt;;</span></a>
<a name="324"><span class="lineNum">     324 </span>            :   /// Typelist of the tensor indices, e.g. types of `ti_a` and `ti_b`</a>
<a name="325"><span class="lineNum">     325 </span>            :   /// in `F(ti_a, ti_b)`</a>
<a name="326"><span class="lineNum">     326 </span><span class="lineCov">          1 :   using args_list = ArgsList&lt;Args...&gt;;</span></a>
<a name="327"><span class="lineNum">     327 </span>            :   /// The number of tensor indices of the `Derived` class</a>
<a name="328"><span class="lineNum">     328 </span><span class="lineCov">          1 :   static constexpr auto num_tensor_indices = tmpl::size&lt;index_list&gt;::value;</span></a>
<a name="329"><span class="lineNum">     329 </span>            : </a>
<a name="330"><span class="lineNum">     330 </span><span class="lineNoCov">          0 :   virtual ~TensorExpression() = 0;</span></a>
<a name="331"><span class="lineNum">     331 </span>            : </a>
<a name="332"><span class="lineNum">     332 </span>            :   /// @{</a>
<a name="333"><span class="lineNum">     333 </span>            :   /// Derived is casted down to the derived class. This is enabled by the</a>
<a name="334"><span class="lineNum">     334 </span>            :   /// [CRTP](https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern)</a>
<a name="335"><span class="lineNum">     335 </span>            :   ///</a>
<a name="336"><span class="lineNum">     336 </span>            :   /// \returns const TensorExpression&lt;Derived, DataType, Symm, IndexList,</a>
<a name="337"><span class="lineNum">     337 </span>            :   /// ArgsList&lt;Args...&gt;&gt;&amp;</a>
<a name="338"><span class="lineNum">     338 </span><span class="lineCov">          1 :   SPECTRE_ALWAYS_INLINE const auto&amp; operator~() const {</span></a>
<a name="339"><span class="lineNum">     339 </span>            :     return static_cast&lt;const Derived&amp;&gt;(*this);</a>
<a name="340"><span class="lineNum">     340 </span>            :   }</a>
<a name="341"><span class="lineNum">     341 </span>            :   /// @}</a>
<a name="342"><span class="lineNum">     342 </span>            : };</a>
<a name="343"><span class="lineNum">     343 </span>            : </a>
<a name="344"><span class="lineNum">     344 </span>            : template &lt;typename Derived, typename DataType, typename Symm,</a>
<a name="345"><span class="lineNum">     345 </span>            :           typename... Indices, template &lt;typename...&gt; class ArgsList,</a>
<a name="346"><span class="lineNum">     346 </span>            :           typename... Args&gt;</a>
<a name="347"><span class="lineNum">     347 </span>            : TensorExpression&lt;Derived, DataType, Symm, tmpl::list&lt;Indices...&gt;,</a>
<a name="348"><span class="lineNum">     348 </span>            :                  ArgsList&lt;Args...&gt;&gt;::~TensorExpression() = default;</a>
<a name="349"><span class="lineNum">     349 </span>            : /// @}</a>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.14</a></td></tr>
  </table>
  <br>

</body>
</html>
